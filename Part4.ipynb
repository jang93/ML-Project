{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "train_file = \"./train\"\n",
    "dev_in = \"./dev.in\"\n",
    "topk_out = \"./dev.p4.out\"\n",
    "\n",
    "\n",
    "def emission_params(train_file):\n",
    "    with open(train_file, encoding = 'utf-8') as file:\n",
    "        emission_count= {}\n",
    "        label_count={}\n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(line.split())!=0:\n",
    "                #add 1 to count of (Xi, Yi)\n",
    "                word = pair[0]\n",
    "                sentiment = pair[1]\n",
    "                if word in emission_count.keys():\n",
    "                    if sentiment in emission_count[word].keys():\n",
    "                        emission_count[word][sentiment] +=1\n",
    "                    else:\n",
    "                        sentiments = emission_count[word]\n",
    "                        sentiments[sentiment] = 1\n",
    "                else:\n",
    "                    sentiment_count = {}\n",
    "                    sentiment_count[sentiment] = 1\n",
    "                    emission_count[word]=sentiment_count\n",
    "    \n",
    "                #add 1 to count of label Yi\n",
    "                if sentiment in label_count.keys():\n",
    "                    label_count[sentiment]+=1\n",
    "                else:\n",
    "                    label_count[sentiment]=1\n",
    "        for keya in emission_count.keys():\n",
    "            for keyb in emission_count[keya].keys():\n",
    "                emission_count[keya][keyb]/=(label_count[keyb]+1)\n",
    "        new_word = {}\n",
    "        for key in label_count.keys():\n",
    "            new_word[key] = 1/(label_count[key]+1)\n",
    "        emission_count['new_word'] = new_word\n",
    "       \n",
    "        return (emission_count,label_count)\n",
    "    \n",
    "def transition_params(train_file):\n",
    "    transition_count= {}\n",
    "    state_count={}\n",
    "    prev = 'START'\n",
    "    end = 'STOP'\n",
    "    state_count[prev] = 0\n",
    "    state_count[end] = 0\n",
    "    transition_count[end] = {}\n",
    "    with open(train_file, encoding = 'utf-8') as file:    \n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(pair)!= 0:\n",
    "                sentiment = pair[1]\n",
    "                # add prev to sentiment transition count\n",
    "                if sentiment in transition_count.keys():\n",
    "                    sentiment_list = transition_count[sentiment]\n",
    "                    if prev in sentiment_list.keys():\n",
    "                        sentiment_list[prev] += 1\n",
    "                    else:\n",
    "                        sentiment_list[prev] = 1\n",
    "                else:\n",
    "                    new_sentiment = {}\n",
    "                    new_sentiment[prev] = 1\n",
    "                    transition_count[sentiment] = new_sentiment\n",
    "\n",
    "                # add to start and stop state counts\n",
    "                if prev == 'START':\n",
    "                    state_count[prev] += 1\n",
    "                    state_count[end] += 1\n",
    "\n",
    "                # add to state count  \n",
    "                if sentiment in state_count.keys():\n",
    "                    state_count[sentiment]+=1\n",
    "                else:\n",
    "                    state_count[sentiment]=1\n",
    "              \n",
    "                prev = sentiment\n",
    "\n",
    "            else:\n",
    "                sentiment_list = transition_count[end]\n",
    "                if prev in sentiment_list.keys():\n",
    "                    sentiment_list[prev] +=1\n",
    "                else:\n",
    "                    sentiment_list[prev] =1   \n",
    "                prev = 'START'\n",
    "    for V in transition_count.keys():\n",
    "        for U in transition_count[V].keys():\n",
    "            transition_count[V][U] /= state_count[U]\n",
    "    return transition_count\n",
    "\n",
    "def viterbi_algo_topk(test_file, output_file, transition_params, emission_params, labels, top_k, i_th):\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        sentence = []\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                sentence.append(line.split()[0])\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            nodes = calculate_topk_node_scores(s,transition_params, emission_params, labels, top_k)\n",
    "            labelled_sentence = backtracking_topk(s,nodes, i_th)\n",
    "            for word in labelled_sentence:\n",
    "                ofile.write(word+'\\n')\n",
    "            ofile.write(\"\\n\")\n",
    "\n",
    "\n",
    "def calculate_topk_node_scores(s, transition_params, emission_params, labels, top_k):\n",
    "    nodes = {}\n",
    "    #base case\n",
    "    nodes[0] = {'START':[[1,'nil',0]]}\n",
    "    #recursive\n",
    "    for k in range (1, len(s)+1): #for each word\n",
    "        X = s[k-1]\n",
    "        for V in labels.keys(): #for each node\n",
    "            prev_nodes_dict = nodes[k-1] #access prev nodes\n",
    "            #emission params\n",
    "            if X in emission_params.keys():\n",
    "                emission_labels = emission_params[X]\n",
    "\n",
    "                if V in emission_labels:\n",
    "                    b = emission_labels[V]\n",
    "                else:\n",
    "                    b = 0\n",
    "            else:\n",
    "                b = emission_params['new_word'][V]  \n",
    "            scores = []\n",
    "            for U in prev_nodes_dict.keys():\n",
    "                #transitionparams\n",
    "                prev_states = transition_params[V]\n",
    "                if U in prev_states.keys():\n",
    "                    a = prev_states[U]\n",
    "                else:\n",
    "                    a = 0\n",
    "                index = 0\n",
    "                for prev_k_nodes in prev_nodes_dict[U]:\n",
    "                    #prev node score\n",
    "                    score = prev_k_nodes[0]*a*b\n",
    "                    scores.append([score, U, index])\n",
    "                    index += 1\n",
    "            \n",
    "            #take top k scores\n",
    "            scores.sort(key=lambda x: x[0],reverse=True)\n",
    "            topk_scores = scores[:top_k]\n",
    "            if k in nodes.keys():\n",
    "                nodes[k][V] = topk_scores\n",
    "            else:\n",
    "                new_dict = {V:topk_scores}\n",
    "                nodes[k] = new_dict\n",
    "            \n",
    "    #end case\n",
    "    prev_nodes_dict = nodes[len(s)]\n",
    "    scores = []\n",
    "    for U in prev_nodes_dict.keys():\n",
    "        #transition\n",
    "        prev_states = transition_params['STOP']\n",
    "        if U in prev_states.keys():\n",
    "            a = prev_states[U]\n",
    "        else:\n",
    "            a = 0\n",
    "        #prev node score\n",
    "        index = 0\n",
    "        for prev_k_nodes in prev_nodes_dict[U]:\n",
    "            score = prev_k_nodes[0]*a\n",
    "            scores.append([score, U, index])\n",
    "            index += 1\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    topk_scores = scores[:top_k]\n",
    "    indiv_node = {'STOP': topk_scores}\n",
    "    nodes[len(s)+1]=indiv_node\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "def backtracking_topk(s, nodes, i_th):\n",
    "    prev_state = 'STOP'\n",
    "    prev_index = 0\n",
    "    for i in range(len(s)+1, 1,-1):\n",
    "        if i==len(s)+1:\n",
    "            prev_node = nodes[i][prev_state][i_th-1]\n",
    "        else:\n",
    "            prev_node = nodes[i][prev_state][prev_index]\n",
    "        prev_state = prev_node[1]\n",
    "        prev_index = prev_node[2]\n",
    "        s[i-2] += \" \"+prev_state\n",
    "    return s\n",
    "\n",
    "import sys\n",
    "top_k = int(sys.argv[1])\n",
    "i_th = int(sys.argv[2])\n",
    "suffix = \"\"\n",
    "\n",
    "if i_th == 1:\n",
    "    suffix = \"st\"\n",
    "elif i_th == 2:\n",
    "    suffix =\"nd\"\n",
    "elif i_th == 3:\n",
    "    suffix = \"rd\"\n",
    "else:\n",
    "    suffix = \"th\"\n",
    "    \n",
    "print(\"Start program\")\n",
    "# obtain e_params\n",
    "print(\"obtaining emission params\")\n",
    "e_params, label_count = emission_params(train_file)\n",
    "# obtain t_params\n",
    "print(\"obtaining transition params\")\n",
    "t_params = transition_params(train_file)\n",
    "# perform viterbi to find top k scores and output ith best output\n",
    "print(\"finding top \"+str(top_k)+\" best output sequences. Outputting \"+str(i_th)+suffix+\" best sequence.\")\n",
    "viterbi_algo_topk(dev_in, topk_out, t_params, e_params, label_count, top_k, i_th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
